# -*- coding: utf-8 -*-
"""Copia de Práctica Análisis de Datos.ipynb

Automatically generated by Colab.

URL: https://www.kaggle.com/datasets/rupakroy/online-payments-fraud-detection-dataset/data
"""
import pandas as pd
data = pd.read_csv(output)

data.head(6)

total_shape = data.shape
print("Número de filas y columnas: ", total_shape)
data.describe().T

"""*   Step is the duration in hours of the transfer
*   Amount is the quantity of the money of the transfer
*   OldbalanceOrig is the old balance from the emisor account
*   NewbalanceOrig is the new balance from the emisor account
*   OldbalanceDest is the old balance from the receptor account
*   NewbalanceDestis the new balance from the receptor account
*   Isfraud: is the target column which determines if the transfer was fraud or not
*   isFlaggedFraud: is what the company thought.

Ovbviously the fraud cases are less than the non fraud ones. Only a few are flagged as fraud, so that column is unuseful.
"""

data.info()

data.isnull().sum() # isnull() devuelve un DataFrame o Series de booleanos (True=1=nulo). sum() suma todos los valores por cada columna de un DataFrame.

"""# Análisis exploratorio de datos

It should be done a cleaning of the data that has the same value 0 in old balance and new balance. But for this task we are going to trust the dataset in that sense.
"""



data.head(6)

"""We check the type of movements and the frequency"""

data["type"].unique() # Lista de valores únicos en la columna

import seaborn as sns
import matplotlib.pyplot as plt

print(data["type"].value_counts())
plt.figure(figsize=(6, 4))
#sns.histplot(data["type"].value_counts(), bins=20, color='skyblue')
data["type"].value_counts().plot(kind='bar')
plt.title('Distribution of Movements')
plt.xlabel('Type transfer')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

data["isFraud"].value_counts()

data["isFlaggedFraud"].value_counts()

"""This is for checking particular conditions"""

negative_value= data[(data["amount"] > data["oldbalanceOrg"])&(data["isFraud"] == 1)]
negative_value= data[(data["amount"] > data["oldbalanceOrg"])]
negative_value= data[(data["oldbalanceDest"] == data["newbalanceDest"])]
print(negative_value.shape)
print(negative_value)

fraud= data[(data["isFraud"] == 1) ]

non_fraud= data[(data["isFraud"] != 1) ]

"""Most of the fraud cases are cash out and transfer, so we can reduce the size of the dataset"""

import seaborn as sns
import matplotlib.pyplot as plt
print(fraud["type"].value_counts())
plt.figure(figsize=(6, 4))
fraud["type"].value_counts().plot(kind='bar')
plt.title('Distribution of fraudes cases')
plt.xlabel('Type transfer')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# Data reduce

We only have fraud cases for cash_out and transfer. So we can reduce the dataset
"""

data_reducee= data[(data["type"] == "CASH_OUT") | (data["type"] == "TRANSFER")  ]

data_reduce = pd.get_dummies(data_reducee, columns=['type'], prefix='') # Te sustituye automáticamente la columna
data_reduce.head()



"""I change the id to number for being easier for the algorithm to work with. However, this can be confused if he interprets the aoumnts."""

import string
letra_a_numero= {letra: index + 1 for index, letra in enumerate(string.ascii_uppercase)} #letra_a_numero = {'A': 1, 'B': 2, 'C': 3}

# Función para reemplazar la letra por el número correspondiente
def reemplazar_letra_por_numero(identidad):
    if identidad[0] in letra_a_numero:
        return str(letra_a_numero[identidad[0]]) + identidad[1:]
    else:
        return identidad

# Aplicar la función a la columna 'identidad'
data_reduce.loc[:, 'nameOrig'] = data_reduce['nameOrig'].apply(reemplazar_letra_por_numero)
data_reduce.loc[:, 'nameDest'] = data_reduce['nameDest'].apply(reemplazar_letra_por_numero)
data_reduce['nameOrig'] = data_reduce['nameOrig'].astype('float64')
data_reduce['nameDest'] = data_reduce['nameDest'].astype('float64')

data_reduce['_CASH_OUT'] = data_reduce['_CASH_OUT'].astype(int)
data_reduce['_TRANSFER'] = data_reduce['_TRANSFER'].astype(int)
data_reduce.head()
data_reduce = data_reduce.drop(columns="nameOrig")
data_reduce = data_reduce.drop(columns="nameDest")

data_reduce['step_day'] = [1 if hora % 24 <= 8 else 0 for hora in data_reduce["step"]]
#data_reduce['step_day'] = [hora % 24  for hora in data_reduce["step"]]

"""I divided in fraud cases and non fraud ones to get balanced classes."""

fraud_cases= data_reduce[(data_reduce["isFraud"] == 1) ]

non_fraud_cases= data_reduce[(data_reduce["isFraud"] != 1) ]

#fraud_cases["type"].value_counts()

print(fraud_cases["step"].value_counts())
plt.figure(figsize=(6, 4))
#sns.histplot(data["type"].value_counts(), bins=200, color='skyblue')
plt.hist(fraud_cases["step"], bins=24, color='#3390FF')
plt.title('fraud cases')
plt.xlabel('Step(hours)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()



"""it is clear that the steps hours are the days. So we take everything and we let them in terms of 24 hours"""

print(non_fraud_cases["step"].value_counts())
plt.figure(figsize=(6, 4))
#sns.histplot(data["type"].value_counts(), bins=200, color='skyblue')
plt.hist(non_fraud_cases["step"], bins=120, color='#3390FF')
plt.title('Non fraud cases')
plt.xlabel('Step(hours)')
plt.ylabel('Frequency')
#plt.xlim(195,220)
plt.grid(True)
plt.show()

"""Obviuously the non fraud cases are made during the day."""

print(fraud_cases["step"].value_counts())
plt.figure(figsize=(6, 4))
#sns.histplot(data["type"].value_counts(), bins=20, color='skyblue')
plt.hist([hora % 24 for hora in non_fraud_cases["step"]], bins=24, color='#3390FF')
plt.title('Non fraud cases')
plt.xlabel('Step(hours)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
a=[hora % 24 for hora in non_fraud_cases["step"]]
print(a)

"""However, the fraud cases are independent of the hour."""

print(fraud_cases["step"].value_counts())
plt.figure(figsize=(6, 4))
#sns.histplot(data["type"].value_counts(), bins=20, color='skyblue')
plt.hist([hora % 24 for hora in fraud_cases["step"]], bins=24, color='#3390FF')
plt.title('Fraud cases')
plt.xlabel('Step(hours)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

data_reduce.head()
#test = fraud_cases.sort(['amount'], ascending=[False])
data_reduce['step_day'].unique()

print(fraud_cases["step"].value_counts())
import numpy as np
plt.figure(figsize=(6, 4))
#sns.histplot(data["type"].value_counts(), bins=20, color='skyblue')
MIN, MAX = 1, fraud_cases["amount"].max()

plt.figure()
plt.hist(fraud_cases["amount"], bins = 10 ** np.linspace(np.log10(MIN), np.log10(MAX)))
#plt.hist(non_fraud_cases["amount"], bins=1000, color='#3390FF')
plt.title('fraud cases')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""Here we see that the 33% of the movements during the night are fraud. However during the day only 1%"""

# Crear una tabla de contingencia
contingency_table = pd.crosstab(data_reduce['step_day'], data_reduce['isFraud'])
print(contingency_table)
import seaborn as sns
# Visualizar la relación
sns.countplot(x='step_day', hue='isFraud', data=data_reduce)
plt.title('Distribution of Day/Night Transactions by Fraud Status')
plt.show()

"""# Selection of characteristics

I look the correlation between columns and I do the difference between fraud cases and non fraud ones(balanced classes).
"""

import seaborn as sns
nf=7000
nnf=7000
databalanced1=fraud_cases.sample(nf)
databalanced2=non_fraud_cases.sample(nnf)
databalanced=pd.concat([databalanced1, databalanced2], axis=0)
numeric_columns = databalanced.select_dtypes(include=['number']).columns.tolist()
print(numeric_columns)
correlation_matrix_data = databalanced[numeric_columns].corr()
plt.figure(figsize=(8, 4))
plt.figure(figsize=(8, 4))
sns.heatmap(correlation_matrix_data, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de Correlación')
plt.show()

"""Fraud has a bit of correlation with oldbalanceorg"""

import seaborn as sns
numeric_columns = fraud_cases.select_dtypes(include=['number']).columns.tolist()
print(numeric_columns)
correlation_matrix_fraud = fraud_cases[numeric_columns].corr()
plt.figure(figsize=(8, 4))
plt.figure(figsize=(8, 4))
sns.heatmap(correlation_matrix_fraud, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de Correlación Fraude')
plt.show()

import seaborn as sns
numeric_columns = non_fraud_cases.select_dtypes(include=['number']).columns.tolist()
print(numeric_columns)
correlation_matrix_non_fraud = non_fraud_cases[numeric_columns].corr()
plt.figure(figsize=(8, 4))
plt.figure(figsize=(8, 4))
sns.heatmap(correlation_matrix_non_fraud, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de Correlación no fraude')
plt.show()

"""These are the correlation values for non fraud. So we do the difference"""

differencefraud=correlation_matrix_fraud-correlation_matrix_non_fraud
plt.figure(figsize=(8, 4))
plt.figure(figsize=(8, 4))
sns.heatmap(differencefraud, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de Correlación = Fraud - non fraude')
plt.show()

"""Negative values, correlation for non fraud and positive values correlation for fraud. When there is colinearity between the amount and the old balance, there is correlation with the fraud cases. That is an indirect correlation.
The most direct correlation is step day.
"""

num_columns = data_reduce.select_dtypes(include=["int64","float64"]).columns.tolist()
print("Columnas numéricas: ", num_columns)
num_columns.remove("isFlaggedFraud")
cat_columns = data_reduce.select_dtypes(include=["object"]).columns.tolist()
print("Columnas categóricas: ", cat_columns)

target_column = "isFraud"

num_pred_columns = num_columns
num_pred_columns.remove(target_column)
print("Columnas numéricas predictoras: ", num_pred_columns)

"""# LogisticRegression unbalanced and weight

The first funcion for studying is the logisticRegression as we have a bynary response in the target column.  We take all the dataset and we balanced the weight with a function. In theory, for unbalanced classes is the best strategy.
"""



num_columns = data_reduce.select_dtypes(include=["int64","float64"]).columns.tolist()
print("Columnas numéricas: ", num_columns)
num_columns.remove("isFlaggedFraud")
cat_columns = data_reduce.select_dtypes(include=["object"]).columns.tolist()
print("Columnas categóricas: ", cat_columns)
num_pred_columns = num_columns
num_pred_columns.remove(target_column)
print("Columnas numéricas predictoras: ", num_pred_columns)

num_columns = df_day.select_dtypes(include=["int64","float64"]).columns.tolist()
print("Columnas numéricas: ", num_columns)
num_columns.remove("isFlaggedFraud")
cat_columns = df_day.select_dtypes(include=["object"]).columns.tolist()
print("Columnas categóricas: ", cat_columns)
num_pred_columns = num_columns
num_pred_columns.remove(target_column)
print("Columnas numéricas predictoras: ", num_pred_columns)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
"""

# CONSTRUCCIÓN DEL PIPELINE
# Transformador para columnas numéricas.
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler()),
])

# Transformador para columnas categóricas.
# Es importante indicarle como tratar nuevos datos una vez realizada la codificación. Esto es útil cuando existe la posibilidad
# de que los datos nuevos contengan categorías que no se vieron durante el entrenamiento.
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Integramos ambos en un preprocesador con ColumnTransformer, indicando el transformador y la lista de columnas a aplicar.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_pred_columns),
        ('cat', categorical_transformer, cat_pred_columns)
    ]
)

def weight_step_day(X):
    X_copy = X.copy()
    X_copy[:, -1] = X_copy[:, -1] * 100  # Suponiendo que 'step_day' es la última columna
    return X_copy

"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
#df_normalized = minmax_scaler.fit_transform(data_scaled[['Average_IQ']])

from sklearn.preprocessing import StandardScaler, FunctionTransformer
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Integramos ambos en un preprocesador con ColumnTransformer, indicando el transformador y la lista de columnas a aplicar.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_pred_columns)
    ]
)

# CREACIÓN DEL PIPELINE CON REGRESIÓN LOGÍSTICA
model = Pipeline([
    ('preprocessor', preprocessor),
    ('log_regressor', LogisticRegression(class_weight='balanced', penalty='l2'))  # Regresión logística
])





mask_day = data_reduce['step_day'] == 0
mask_night = data_reduce['step_day'] == 1

# Crear DataFrames separados
df_day = data_reduce[mask_day]
df_night = data_reduce[mask_night]

from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV

# DIVISIÓN ENTRE COLUMNAS PREDICTIVAS Y OBJETIVO
# Separar colunas predictoras de columna objetivo
X = data_reduce.drop(columns=target_column)
X = X.drop(columns="isFlaggedFraud")
ndata=500_000+8000
X=X[:ndata]
y = data_reduce[target_column][:ndata]

X = df_day.drop(columns=target_column)
X = X.drop(columns="isFlaggedFraud")

X=X
y = df_day[target_column]

# DIVISIÓN ENTRE DATOS DE ENTRENAMIENTO Y DATOS DE TESTEO
# Seleccionamos una proporción de 80% de los datos para entrenamiento y 20% para el testeo.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y,random_state=4)

# VALIDACIÓN CRUZADA
# Realizar K-Fold Cross-Validation con 5 folds.
import numpy as np
cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring="accuracy")
print(f'Puntuaciones de Validación Cruzada: {np.round(cv_scores,2)}')
print(f'Promedio de las puntuaciones de Validación Cruzada: {round(np.mean(cv_scores),2)}')
print(f'Desviación Típica de las puntuaciones de Validación Cruzada: {round(np.std(cv_scores),2)}')

# ENTRENAMIENTO DEL MODELO
# Entrenar el modelo final con todos los datos de entrenamiento.

#X_train=X_train[:ndata]
#y_train=y_train[:ndata]
X_train=X_train
y_train=y_train
model.fit(X_train, y_train)

# TESTEO DEL MODELO
y_pred = model.predict(X_test)  # predict() devuelve las asignaciones a clases en función de la probabilidad.

# COEFICIENTES FINALES
# Acceder al modelo de regresión dentro del pipeline
log_regressor = model.named_steps['log_regressor'] # Debe ser el mismo nombre indicado al declarar el pipeline

# Obtener los coeficientes y el interceptor
coefficients = np.round(log_regressor.coef_)
intercept = np.round(log_regressor.intercept_)
feature_names = preprocessor.get_feature_names_out() # Sacamos los nombres de las columnas.
coef_dict = dict(zip(feature_names, coefficients.flatten())) # Asociando los coeficientes con los nombres de las características
print("Interceptor del modelo:", intercept)

print("Coeficientes finales del modelo:", coef_dict)
"""
sorted_coef_dict = dict(sorted(coef_dict.items(), key=lambda item: item[1]))
for feature, coef in sorted_coef_dict.items():
    print(f'{feature}: {coef}')
    """

"""The factors of newbalance are very important. Stepday is 0 because is a category value, so in standarscaler is difficult to outstand. Also as it was shown in the correlation matrix, the strongest one was between amount and old balance org"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
# EVALUACIÓN DEL MODELO
accuracy = round(accuracy_score(y_test, y_pred),2)
precision = round(precision_score(y_test, y_pred),2)
recall = round(recall_score(y_test, y_pred),2)
f1 = round(f1_score(y_test, y_pred),2)
conf_matrix = confusion_matrix(y_test, y_pred)

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

# Cálculo de la especificidad
specificity = tn / (tn + fp)
print("Exactitud:", accuracy)
print("Precisión:", precision)
print("Sensibilidad fraude:", recall)
print("Specificity detecte los que no son:", specificity)
print("F1 Score:", f1)
print("Matriz de Confusión:\n", conf_matrix)
print((y_test))
columns_name = data_reduce.columns.tolist()
X_test_frame = pd.DataFrame(X_test, columns=data_reduce.columns[:-1])
#X_test_frame.drop(target_column)
#y_test_frame = pd.DataFrame(y_test, columns="y_test")
#y_pred_frame = pd.DataFrame(y_pred, columns="y_pred")
#confusion_matrix_pred=pd.concat([X_test_frame,y_test, y_pred], axis=1)
df = pd.DataFrame(X_test, columns=data.columns[:-1])  # Usa nombres de columnas apropiados para tus características
df['y_test'] = y_test
df['y_pred'] = y_pred
# Filtrar los casos donde y_pred es 0 y y_test es 1
filtered_df = df[(df['y_pred'] == 0) & (df['y_test'] == 1)]

"""

*   Only daily movements: sensibilidad 91 specifity 93.7
*   Only night movements: sensibilidad 93 specificidad 95


*   All together: Sensibilidad 92 specificidad 94.8


It's not worthy separate the dataset by two because the training sample is smaller so the results are lower. However the results are higher for the night, maybe because the classes are more balanced.




"""



# Calcular la curva ROC
y_prob = model.predict_proba(X_test)[:, 1] # predict_proba() devuelve las probabilidades para la clase positiva 1.
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label='Área bajo la curva = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Especificidad')
plt.ylabel('Sensibilidad')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

"""# Logistic Regression balanced classes and study

I optimize the parameters for this model with balanced classes
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
# Transformador para columnas numéricas.

nf=6000
nnf=6000
databalanced1=fraud_cases.sample(nf)
databalanced2=non_fraud_cases.sample(nnf)
databalanced=pd.concat([databalanced1, databalanced2], axis=0)
databalanced = databalanced.drop(columns="newbalanceDest")
databalanced = databalanced.drop(columns="oldbalanceOrg")
num_columns = databalanced.select_dtypes(include=["int64","float64"]).columns.tolist()
print("Columnas numéricas: ", num_columns)
num_columns.remove("isFlaggedFraud")
cat_columns = databalanced.select_dtypes(include=["object"]).columns.tolist()
print("Columnas categóricas: ", cat_columns)

target_column = "isFraud"

num_pred_columns = num_columns
num_pred_columns.remove(target_column)
print("Columnas numéricas predictoras: ", num_pred_columns)
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Integramos ambos en un preprocesador con ColumnTransformer, indicando el transformador y la lista de columnas a aplicar.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_pred_columns)
    ]
)

# CREACIÓN DEL PIPELINE CON REGRESIÓN LOGÍSTICA
model = Pipeline([
    ('preprocessor', preprocessor),
    ('log_regressor', LogisticRegression())  # Regresión logística
])

data_reduce.info()
print(num_pred_columns)

"""Here I have two cases because there is a clear mayority class.
The first case is to take all the data and the care that the portion of non fraud cases is the same in the train dataset than in the test dataset. This is get with the function stratify.
The second case is manually get balanced class for the training model, and then take the rest of the dataset for the test and prediction. It is very important not using the fraud cases from the train dataset in the prediction, because you can get an overfitting if the model memorize the cases where the transfers were fraud.
"""

from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV



print(databalanced1.shape)
print(databalanced2.shape)

X = databalanced.drop(columns=target_column)
X = X.drop(columns="isFlaggedFraud")
y = databalanced[target_column]
# DIVISIÓN ENTRE DATOS DE ENTRENAMIENTO Y DATOS DE TESTEO
# Seleccionamos una proporción de 80% de los datos para entrenamiento y 20% para el testeo.
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train=X
y_train=y

# Merge con indicator
df_merged = data_reduce.merge(databalanced,  how='left', indicator=True)
# Filtrar filas que están solo en el DataFrame original
data_test = df_merged[df_merged['_merge'] == 'left_only'].drop(columns='_merge')
X_test= data_test.drop(columns=target_column)
X_test = X_test.drop(columns="isFlaggedFraud")
y_test = data_test[target_column]
ndata=5_000_0000
X_test=X_test[:ndata]
y_test=y_test[:ndata]
print(X.shape)
print(X)
print(X.shape)
print(y)
from sklearn.model_selection import train_test_split, cross_val_score


print("Tamaño datos de entrenamiento:", X_train.shape)
print("Tamaño datos de testeo:", X_test.shape)

# VALIDACIÓN CRUZADA
# Realizar K-Fold Cross-Validation con 5 folds.
import numpy as np
cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring="accuracy")
print(f'Puntuaciones de Validación Cruzada: {np.round(cv_scores,2)}')
print(f'Promedio de las puntuaciones de Validación Cruzada: {round(np.mean(cv_scores),2)}')
print(f'Desviación Típica de las puntuaciones de Validación Cruzada: {round(np.std(cv_scores),2)}')

# ENTRENAMIENTO DEL MODELO
# Entrenar el modelo final con todos los datos de entrenamiento.

#X_train=X_train[:ndata]
#y_train=y_train[:ndata]
X_train=X_train
y_train=y_train
model.fit(X_train, y_train)

# TESTEO DEL MODELO
y_pred = model.predict(X_test)  # predict() devuelve las asignaciones a clases en función de la probabilidad.

"""Here I want a big value in step day, so I am changing the weights in the pipeline for that."""

# COEFICIENTES FINALES
# Acceder al modelo de regresión dentro del pipeline
log_regressor = model.named_steps['log_regressor'] # Debe ser el mismo nombre indicado al declarar el pipeline

# Obtener los coeficientes y el interceptor
coefficients = np.round(log_regressor.coef_)
intercept = np.round(log_regressor.intercept_)
feature_names = preprocessor.get_feature_names_out() # Sacamos los nombres de las columnas.
coef_dict = dict(zip(feature_names, coefficients.flatten())) # Asociando los coeficientes con los nombres de las características
print("Interceptor del modelo:", intercept)

print("Coeficientes finales del modelo:", coef_dict)
"""
sorted_coef_dict = dict(sorted(coef_dict.items(), key=lambda item: item[1]))
for feature, coef in sorted_coef_dict.items():
    print(f'{feature}: {coef}')
    """

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
# EVALUACIÓN DEL MODELO
accuracy = round(accuracy_score(y_test, y_pred),2)
precision = round(precision_score(y_test, y_pred),2)
recall = round(recall_score(y_test, y_pred),2)
f1 = round(f1_score(y_test, y_pred),2)
conf_matrix = confusion_matrix(y_test, y_pred)

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

# Cálculo de la especificidad
specificity = tn / (tn + fp)
print("Exactitud:", accuracy)
print("Precisión:", precision)
print("Sensibilidad fraude:", recall)
print("Specificity detecte los que no son:", specificity)
print("F1 Score:", f1)
print("Matriz de Confusión:\n", conf_matrix)
print((y_test))
columns_name = data_reduce.columns.tolist()
X_test_frame = pd.DataFrame(X_test, columns=data_reduce.columns[:-1])
#X_test_frame.drop(target_column)
#y_test_frame = pd.DataFrame(y_test, columns="y_test")
#y_pred_frame = pd.DataFrame(y_pred, columns="y_pred")
#confusion_matrix_pred=pd.concat([X_test_frame,y_test, y_pred], axis=1)
df = pd.DataFrame(X_test, columns=data.columns[:-1])  # Usa nombres de columnas apropiados para tus características
df['y_test'] = y_test
df['y_pred'] = y_pred
# Filtrar los casos donde y_pred es 0 y y_test es 1
filtered_df = df[(df['y_pred'] == 0) & (df['y_test'] == 1)]

"""The results of the data are worse than the previous example."""



# Calcular la curva ROC
y_prob = model.predict_proba(X_test)[:, 1] # predict_proba() devuelve las probabilidades para la clase positiva 1.
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label='Área bajo la curva = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Especificidad')
plt.ylabel('Sensibilidad')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

# DIVISIÓN ENTRE COLUMNAS PREDICTIVAS Y OBJETIVO
# Separar colunas predictoras de columna objetivo
def optimizar_logit(nf,nnf,ndata,data_reduce,fraud_cases,non_fraud_cases,target_column,model):
  databalanced1=fraud_cases.sample(nf)
  databalanced2=non_fraud_cases.sample(nnf)
  databalanced=pd.concat([databalanced1, databalanced2], axis=0)

  X = databalanced.drop(columns=target_column)
  X = X.drop(columns="isFlaggedFraud")
  y = databalanced[target_column]
  X_train=X
  y_train=y
# Merge con indicator
  df_merged = data_reduce.merge(databalanced,  how='left', indicator=True)
  # Filtrar filas que están solo en el DataFrame original
  data_test = df_merged[df_merged['_merge'] == 'left_only'].drop(columns='_merge')
  X_test= data_test.drop(columns=target_column)
  X_test = X_test.drop(columns="isFlaggedFraud")
  y_test = data_test[target_column]
  ndata=200_0000
  X_test=X_test[:ndata]
  y_test=y_test[:ndata]

  from sklearn.model_selection import train_test_split, cross_val_score
  cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring="accuracy")

  X_train=X_train[:ndata]
  y_train=y_train[:ndata]
  model.fit(X_train, y_train)

  y_pred = model.predict(X_test)  # predict() devuelve las asignaciones a clases en función de la probabilidad.

  # COEFICIENTES FINALES
  # Acceder al modelo de regresión dentro del pipeline
  log_regressor = model.named_steps['log_regressor'] # Debe ser el mismo nombre indicado al declarar el pipeline

  # Obtener los coeficientes y el interceptor
  coefficients = np.round(log_regressor.coef_)
  intercept = np.round(log_regressor.intercept_)
  feature_names = preprocessor.get_feature_names_out() # Sacamos los nombres de las columnas.
  coef_dict = dict(zip(feature_names, coefficients.flatten())) # Asociando los coeficientes con los nombres de las características

  from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
  # EVALUACIÓN DEL MODELO
  accuracy = round(accuracy_score(y_test, y_pred),2)
  precision = round(precision_score(y_test, y_pred),2)
  recall = round(recall_score(y_test, y_pred),2)
  f1 = round(f1_score(y_test, y_pred),2)
  conf_matrix = confusion_matrix(y_test, y_pred)

  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

  specificity = tn / (tn + fp)

  return specificity,recall
Specifity=np.zeros((8, 8))
Recall=np.zeros((8, 8))
for i in range(1,9,1):
  for j in range(1,9,1):
    Specifity[i-1,j-1],Recall[i-1,j-1]=optimizar_logit(i*1000,j*1000,ndata,data_reduce,fraud_cases,non_fraud_cases,target_column,model)

"""Here I did a study about how balance the classes."""

# Crear el colormap
plt.imshow(Specifity, cmap='viridis', interpolation='nearest')

# Añadir colorbar para referencia
plt.colorbar()

# Añadir títulos y etiquetas
plt.title("Specifity(evita los falsos fraude)")
plt.xlabel("1000*n No fraude")
plt.ylabel("1000*n Fraude")

# Mostrar la imagen
plt.show()

# Crear el colormap
plt.imshow(Recall, cmap='viridis', interpolation='nearest')

# Añadir colorbar para referencia
plt.colorbar()

# Añadir títulos y etiquetas
plt.title("Recall detección de fraude")
plt.xlabel("1000*n No fraude")
plt.ylabel("1000*n Fraude")

# Mostrar la imagen
plt.show()

# Crear el colormap
plt.imshow(Specifity*Recall, cmap='viridis', interpolation='nearest')

# Añadir colorbar para referencia
plt.colorbar()

# Añadir títulos y etiquetas
plt.title("Colormap de Matriz 8x8")
plt.xlabel("1000*n No fraude")
plt.ylabel("1000*n Fraude")

# Mostrar la imagen
plt.show()

# Crear el colormap
plt.imshow(Specifity*Recall*Recall, cmap='viridis', interpolation='nearest')

# Añadir colorbar para referencia
plt.colorbar()

# Añadir títulos y etiquetas
plt.title("Deteccion fraude(preferencia) y verdaderos positivos")
plt.xlabel("1000*n No fraude")
plt.ylabel("1000*n Fraude")

# Mostrar la imagen
plt.show()

"""```
```

# Logistic SMOTE

Here I deicided to increase the fraud cases with k neighbours. However I didn't get any increase in the final prediction.
"""

# DIVISIÓN ENTRE COLUMNAS PREDICTIVAS Y OBJETIVO
# Separar colunas predictoras de columna objetivo

X = data_reduce.drop(columns=target_column)
X = X.drop(columns="isFlaggedFraud")
ndata=5_000_000+8000
X=X[:ndata]
y = data_reduce[target_column]
# DIVISIÓN ENTRE DATOS DE ENTRENAMIENTO Y DATOS DE TESTEO
# Seleccionamos una proporción de 80% de los datos para entrenamiento y 20% para el testeo.
from sklearn.model_selection import train_test_split, cross_val_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y,random_state=4)

print("Tamaño datos de entrenamiento:", X_train.shape)
print("Tamaño datos de testeo:", X_test.shape)

# Transformador para columnas numéricas.
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Integramos ambos en un preprocesador con ColumnTransformer, indicando el transformador y la lista de columnas a aplicar.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_pred_columns)
    ]
)

unique, counts = np.unique(y_train, return_counts=True)
class_counts = dict(zip(unique, counts))
print(f'Número de muestras por clase antes de SMOTE: {class_counts}')
minority_class = min(class_counts, key=class_counts.get)
minority_class_count = class_counts[minority_class]
sampling_strategy = {minority_class: 10 * minority_class_count}

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
# CREACIÓN DEL PIPELINE CON REGRESIÓN LOGÍSTICA
model = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(sampling_strategy=sampling_strategy)),
    ('log_regressor',  LogisticRegression(class_weight='balanced', penalty='l2'))  # Regresión logística
])

# VALIDACIÓN CRUZADA
# Realizar K-Fold Cross-Validation con 5 folds.
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring="accuracy")
print(f'Puntuaciones de Validación Cruzada: {np.round(cv_scores,2)}')
print(f'Promedio de las puntuaciones de Validación Cruzada: {round(np.mean(cv_scores),2)}')
print(f'Desviación Típica de las puntuaciones de Validación Cruzada: {round(np.std(cv_scores),2)}')

# ENTRENAMIENTO DEL MODELO
# Entrenar el modelo final con todos los datos de entrenamiento.
n=200_000
X_train=X_train
y_train=y_train
model.fit(X_train, y_train)

# TESTEO DEL MODELO
y_pred = model.predict(X_test)  # predict() devuelve las asignaciones a clases en función de la probabilidad

# COEFICIENTES FINALES
# Acceder al modelo de regresión dentro del pipeline
log_regressor = model.named_steps['log_regressor'] # Debe ser el mismo nombre indicado al declarar el pipeline

# Obtener los coeficientes y el interceptor
coefficients = np.round(log_regressor.coef_)
intercept = np.round(log_regressor.intercept_)
feature_names = preprocessor.get_feature_names_out() # Sacamos los nombres de las columnas.
coef_dict = dict(zip(feature_names, coefficients.flatten())) # Asociando los coeficientes con los nombres de las características
print("Interceptor del modelo:", intercept)

print("Coeficientes finales del modelo:", coef_dict)
"""
sorted_coef_dict = dict(sorted(coef_dict.items(), key=lambda item: item[1]))
for feature, coef in sorted_coef_dict.items():
    print(f'{feature}: {coef}')
    """

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
# EVALUACIÓN DEL MODELO
accuracy = round(accuracy_score(y_test, y_pred),2)
precision = round(precision_score(y_test, y_pred),2)
recall = round(recall_score(y_test, y_pred),2)
f1 = round(f1_score(y_test, y_pred),2)
conf_matrix = confusion_matrix(y_test, y_pred)

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

# Cálculo de la especificidad
specificity = tn / (tn + fp)
print("Exactitud:", accuracy)
print("Precisión:", precision)
print("Sensibilidad fraude:", recall)
print("Specificity detecte los que no son:", specificity)
print("F1 Score:", f1)
print("Matriz de Confusión:\n", conf_matrix)
print((y_test))
X_test_frame = pd.DataFrame(X_test, columns=databalanced.columns[:-1])
#X_test_frame.drop(target_column)
#y_test_frame = pd.DataFrame(y_test, columns="y_test")
#y_pred_frame = pd.DataFrame(y_pred, columns="y_pred")
#confusion_matrix_pred=pd.concat([X_test_frame,y_test, y_pred], axis=1)
df = pd.DataFrame(X_test, columns=data.columns[:-1])  # Usa nombres de columnas apropiados para tus características
df['y_test'] = y_test
df['y_pred'] = y_pred
# Filtrar los casos donde y_pred es 0 y y_test es 1
filtered_df = df[(df['y_pred'] == 0) & (df['y_test'] == 1)]

"""If we compare to the previous example, the results are the same but obviously the time for training the model is longer.

# Modelos de regresión logística

I did a gridsearchcv for getting the best penalty and parameter C for the logistic regression.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression

# DIVISIÓN ENTRE COLUMNAS PREDICTIVAS Y OBJETIVO
# Separar colunas predictoras de columna objetivo

X = data_reduce.drop(columns=target_column)
X = X.drop(columns="isFlaggedFraud")
y = data_reduce[target_column]
# DIVISIÓN ENTRE DATOS DE ENTRENAMIENTO Y DATOS DE TESTEO
# Seleccionamos una proporción de 80% de los datos para entrenamiento y 20% para el testeo.
from sklearn.model_selection import train_test_split, cross_val_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y,random_state=4)
ndata=200_000
X_train=X_train[:ndata]
y_train=y_train[:ndata]
X_test=X_test[:ndata]
y_test=y_test[:ndata]

# Transformador para columnas numéricas.
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Integramos ambos en un preprocesador con ColumnTransformer, indicando el transformador y la lista de columnas a aplicar.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_pred_columns)
    ]
)

unique, counts = np.unique(y_train, return_counts=True)
class_counts = dict(zip(unique, counts))
print(f'Número de muestras por clase antes de SMOTE: {class_counts}')
minority_class = min(class_counts, key=class_counts.get)
minority_class_count = class_counts[minority_class]
sampling_strategy = {minority_class: 5 * minority_class_count}

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
# CREACIÓN DEL PIPELINE CON REGRESIÓN LOGÍSTICA
model = ImbPipeline([
    ('preprocessor', preprocessor),
    ('log_regressor', LogisticRegression(solver='saga', max_iter=1000, class_weight='balanced'))  # Regresión logística
])#usar solver saga para el l1 y l2

param_grid = {
    'log_regressor__penalty': [ 'l1','l2'],
    'log_regressor__C': [ 0.1, 1, 10,100],
}
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
# Configurar GridSearchCV
grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)

# Entrenar el modelo
grid_search.fit(X_train, y_train)

# Imprimir los mejores parámetros y el mejor puntaje
print(f'Mejores parámetros: {grid_search.best_params_}')
print(f'Mejor puntuación de validación cruzada: {grid_search.best_score_}')

# Evaluar el mejor modelo en el conjunto de prueba
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f'Puntuación en el conjunto de prueba: {test_score}')

"""# Random forest classifier

I tried this model because maybe it could be more efficient with the column step day. That in my opinion is critical.
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer

nf=6000
nnf=6000
databalanced1=fraud_cases.sample(nf)
databalanced2=non_fraud_cases.sample(nnf)
databalanced=pd.concat([databalanced1, databalanced2], axis=0)
num_columns = databalanced.select_dtypes(include=["int64","float64"]).columns.tolist()
print("Columnas numéricas: ", num_columns)
num_columns.remove("isFlaggedFraud")
cat_columns = databalanced.select_dtypes(include=["object"]).columns.tolist()
print("Columnas categóricas: ", cat_columns)

target_column = "isFraud"

num_pred_columns = num_columns
num_pred_columns.remove(target_column)
print("Columnas numéricas predictoras: ", num_pred_columns)

X = databalanced.drop(columns=target_column)
X = X.drop(columns="isFlaggedFraud")
y = databalanced[target_column]
# DIVISIÓN ENTRE DATOS DE ENTRENAMIENTO Y DATOS DE TESTEO
# Seleccionamos una proporción de 80% de los datos para entrenamiento y 20% para el testeo.
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train=X
y_train=y

# Merge con indicator
df_merged = data_reduce.merge(databalanced,  how='left', indicator=True)
# Filtrar filas que están solo en el DataFrame original
data_test = df_merged[df_merged['_merge'] == 'left_only'].drop(columns='_merge')
X_test= data_test.drop(columns=target_column)
X_test = X_test.drop(columns="isFlaggedFraud")
y_test = data_test[target_column]
ndata=5_000_0000
X_test=X_test[:ndata]
y_test=y_test[:ndata]


# CREACIÓN DEL PIPELINE CON SVMs
model = Pipeline([
    ('classifier', RandomForestClassifier(random_state=42, max_depth=None, oob_score=True)) # Random Forest para tareas de clasificación.
])

metaparameter_list = ['classifier__max_features', 'classifier__min_samples_split']
param_grid = {
    metaparameter_list[0]: ['sqrt', 'log2', None], # Las opciones es la raíz
    metaparameter_list[1]: [2, 5, 10],
}

# Configurar el GridSearchCV
grid_search = GridSearchCV(model, param_grid, cv=5, verbose = 1, scoring='f1_weighted')

from scipy.stats import mode

# BÚSQUEDA DE HIPERPARÁMETROS CON VALIDACIÓN ANIDADA.
# Este paso es independiente del entrenamiento posterior, pero es recomendable para evaluar la variabilidad de los modelos en cuanto a evaluación y selección de hiperparámetros.
n_splits = 5
outer_cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)
best_params_list = []
best_scores = []

# Aquí estamos haciendo una Validación Cruzada manualmente. En cada iteración (tantas como número de folds indicados) se calculará un GridSearchCV
# con el conjunto de entrenamiento seleccionado en ese fold.
for train_index, test_index in outer_cv.split(X_train):
    X_train_fold = X_train.iloc[train_index]
    X_test_fold = X_train.iloc[test_index]
    y_train_fold = y_train.iloc[train_index]
    y_test_fold = y_train.iloc[test_index]

    # Ejecutar GridSearchCV
    grid_search.fit(X_train_fold, y_train_fold)

    # Almacenar los mejores parámetros y los mejores resultados en cada split
    best_params_list.append(grid_search.best_params_)
    best_scores.append(grid_search.best_score_)

# Un bucle por fold, indicando el hiperparámetro óptimo y la mejor métrica de error de esa iteración.
for split in range(n_splits):
  for metaparameter in metaparameter_list:
    value = best_params_list[split][metaparameter]
    if isinstance(value, (int, float)):
        print(f'Mejor valor en el fold {split} del {metaparameter} en VC anidada: {round(value, 3)}')
    else:
        print(f'Mejor valor en el fold {split} del {metaparameter} en VC anidada: {value}')
  print(f"Mejor f1-Score ponderada en el fold {split}: {np.round(best_scores[split],2)}\n")

# Un bucle para los estadísticos de cada hiperparámetro
for metaparameter in metaparameter_list:
  values = [value[metaparameter] for value in best_params_list] # Recogemos los diferentes valores que nos devuelve cada fold
  if isinstance(values[0], (int, float)):
    mean = sum(values) / len(values) # Calculamos la media
    std = np.sqrt(sum((value - mean) ** 2 for value in values) / len(values)) # Calculamos la Desviación Típica
    print(f'Promedio de las puntuaciones {metaparameter} en VC anidada: {round(mean,3)}')
    print(f'Desviación Típica de las puntuaciones {metaparameter} en VC anidada: {round(std,3)}\n')
  else:
    try:
      unique_values, counts = np.unique(values, return_counts=True) # Valores y su frecuencia en values
      max_index = np.argmax(counts) # Índice del valor con mayor frecuencia
      mode = unique_values[max_index]
      count = counts[max_index]
      print(f'Moda de las puntuaciones {metaparameter} en VC anidada: {mode}')
      print(f'Frecuencia de la moda de las puntuaciones {metaparameter} en VC anidada: {count}')
    except TypeError:
      print(f'No se pueden comparar valores para {metaparameter}: {values}')

# BÚSQUEDA DE HIPERPARÁMETROS SIN VALIDACIÓN ANIDADA
# Entrenamos ahora con GridSearchCV sin anidar.
grid_search.fit(X_train, y_train)

# Mostrar mejor puntuación y los mejores parámetros. Podemos compararlos a aquellos valores obtenidos en la validación cruzada anidada.
for metaparameter in metaparameter_list:
  value = grid_search.best_params_[metaparameter]
  if isinstance(value, (int, float)):
    print(f'Mejor puntuación del {metaparameter} en VC anidada: {round(grid_search.best_params_[metaparameter],3)}')
  else:
    print(f'Mejor valor del {metaparameter} en VC anidada: {value}')
print("Mejor F1-Score:", np.round(grid_search.best_score_,2))

# TESTEO DEL MODELO
# Recoger el modelo con los mejores hiperparámetros
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve

# EVALUACIÓN DEL MODELO
accuracy = round(accuracy_score(y_test, y_pred), 2)
precision = round(precision_score(y_test, y_pred), 2)
recall = round(recall_score(y_test, y_pred), 2)
f1 = round(f1_score(y_test, y_pred), 2)
conf_matrix = confusion_matrix(y_test, y_pred)
tree_model = best_model.named_steps['classifier']
oob_score = tree_model.oob_score_
specificity = tn / (tn + fp)
print("Exactitud:", accuracy)
print("Precisión:", precision)
print("Sensibilidad fraude:", recall)
print("Specificity detecte los que no son:", specificity)
print("F1 Score:", f1)
print(f"OOB Score: {oob_score:.2f}")
print("Matriz de Confusión:\n", conf_matrix)

print(2708631/(47565+2708631))

# LOCALIZACIÓN DEL PUNTO CRÍTICO
n_estimators_range = range(1, 101) # Rango en el número de estimadores a probar
train_errors = []
test_errors = []

# Extraer los mejores hiperparámetros
best_max_features = grid_search.best_params_['classifier__max_features']
best_min_samples_split = grid_search.best_params_['classifier__min_samples_split'] # Metaparámetros obtenido del mejor modelo obtenido en el GridSearch

# Este bucle entrena y evalua un Random Forest, cada uno con un número distinto de estimadores.
for n in n_estimators_range:
    clf = RandomForestClassifier(
        n_estimators=n,
        max_features = best_max_features,
        min_samples_split = best_min_samples_split,
        random_state=42,
    )
    clf.fit(X_train, y_train)

    # Obtenemos la medida del error, tanto para las muestras de entrenamiento como de testeo.
    train_errors.append(1 - f1_score(y_train, clf.predict(X_train)))
    test_errors.append(1 - f1_score(y_test, clf.predict(X_test)))

# Graficar los errores de entrenamiento y prueba
plt.figure(figsize=(8, 4))
plt.plot(n_estimators_range, train_errors, label='Error de Entrenamiento')
plt.plot(n_estimators_range, test_errors, label='Error de Prueba')
plt.xlabel('Número de Árboles')
plt.ylabel('Error')
plt.title('Error vs. Número de Árboles en el Random Forest')
plt.legend()
plt.show()

"""The model is totally wrong and overfits in everyplace. The accuracy in the train data is perfect and in in test data totally wrong. Maybe it could be optimized better."""

# INTERPRETACIÓN DEL MODELO
# Extraemos las métricas de relevancia de cara característica.
importances = tree_model.feature_importances_
# Crear un DataFrame para visualizar las importancias
feature_importances = pd.DataFrame({'feature': num_pred_columns, 'importance': importances})
feature_importances = feature_importances.sort_values(by='importance', ascending=False)
# Graficar la importancia de las características
plt.figure(figsize=(5, 2))
plt.bar(feature_importances['feature'], feature_importances['importance'])
plt.xlabel('Características')
plt.ylabel('Importancia')
plt.title('Importancia de las Características del Random Forest')
plt.xticks(rotation=45)
plt.show()

"""

```
# Clusters
```

# Clusters"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
data_cluster=data_reduce
data_cluster=data_cluster.drop(columns="isFraud")
data_cluster=data_cluster.drop(columns="isFlaggedFraud")
data_cluster=data_cluster.drop(columns="step")
scaler = StandardScaler()
standardized_data = scaler.fit_transform(data_cluster)
standardized_data[:, -1] = standardized_data[:, -1] * 1
# APLICAR PCA PARA VISUALIZACIÓN
# Inicializar y ajustar el modelo K-Means
n_components = 2
n_clusters=2
kmeans = KMeans(n_clusters= n_clusters, init='k-means++', random_state=42) # Definimos un objeto KMeans.
kmeans.fit(standardized_data) # Generacmos la clusterización.

# Creamos un nuevo DataFrame y guardamos en el las asignaciones a los clústers.
data_kmeans = pd.DataFrame(data_cluster).copy()
data_kmeans["n_cluster"] = kmeans.labels_ # kmeans.labels_ guarda las asignaciones de clusters.
data_kmeans.head()

pca = PCA(n_components=2)
pca.fit(standardized_data)
data_pca = pca.transform(data_kmeans.iloc[:, :-1]) # Aplicar PCA al DataFrame excluyendo la última, que es la que guarda las asignaciones a clústers.
data_pca = pd.DataFrame(data_pca, columns=["PC1","PC2"]) # Generamos
data_pca["n_cluster"] = kmeans.labels_

plt.figure(figsize=(10, 7))
for cluster in data_pca['n_cluster'].unique():
    cluster_data = data_pca[data_pca['n_cluster'] == cluster]
    plt.scatter(cluster_data['PC1'], cluster_data['PC2'], label=f'Cluster {cluster}', alpha=0.5)

plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('PCA')
plt.legend()
plt.show()

"""# Conclusions

The best strategy is logistic regression. Random forest overfits and can not predict anything with the test data. Another option that should be studied would be the neural network. But that maybe is too complex for the goals of this task.
"""